{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"# Work in progress, we are now testing a Stacked version ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) OFFICIAL VERSION USED for the Best Score"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#####1) OFFICIAL VERSION USED for the Best Score######\nimport numpy as np\nimport pandas as pd\n\n#Libraries for feature selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n\n#Path\npath_train = \"/kaggle/input/statistical-learning-sapienza-spring-2020/train/train.csv\"\npath_test = \"/kaggle/input/statistical-learning-sapienza-spring-2020/test/test.csv\"\ntrain = pd.read_csv(path_train)\ntest  = pd.read_csv(path_test)\n\n#Save the 'Id' column\ntrain_ID = train['id']\ntest_ID = test['id']\n\n#Now drop the  'Id' colum since it's unnecessary for the prediction process.\ntrain.drop(\"id\", axis = 1, inplace = True)\ntest.drop(\"id\", axis = 1, inplace = True)\n\n# Reshape train and test\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.tempo.values\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['tempo'], axis=1, inplace=True)\ntrain.drop(['tempo'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"### FEAUTURES SELECTION ### (Which actually improved our score)\nnew = []\nfor i in y_train: \n    i = int(i)\n    new += [i]\nX = np.array(train)\ny = np.array(new)\n\nscaler = MinMaxScaler()\nscaler = scaler.fit(X)\nX = scaler.transform(X)\nX = pd.DataFrame(data=X[0:,0:],\n            index=[i for i in range(X.shape[0])],\n            columns=['f'+str(i) for i in range(X.shape[1])])\n\n#Using SelectKBest for feature selection\nbestfeatures = SelectKBest(score_func=chi2, k=40000)\nfit = bestfeatures.fit(X,y)\n\n\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']   \n\n#Taking the top 209 important features\nbest_columns = (featureScores.nlargest(209,'Score')) \ncandidate_columns = list(best_columns.index.values)\n\n# Let's cut the dataframe\ncolname = all_data.columns[candidate_columns]\ndf1 = all_data[colname]\n\n\n# We only get the cut version \ntrain_cut = df1[:ntrain]\ntest_cut = df1[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBRegression with the best score"},{"metadata":{"trusted":true},"cell_type":"code","source":"### XGBRegression ###\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Various hyper-parameters to tune\nxgb1 = XGBRegressor()\nparameters = {'objective':['reg:squarederror'],\n              'learning_rate': [0.01, 0.03, 0.05 ], #so called `eta` value\n              'max_depth': [7, 9],\n              'min_child_weight': [ 3 ,5],\n              'gamma': [ 0.2 ,0.3,  0.4 ],\n              'colsample_bytree': [ 0.8, 0.9, 1] ,\n              'n_estimators': [500]}\n\n# GridSearch for hyperparameters tuning\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs = -1,\n                        verbose=True)\n\nxgb_grid.fit(train_cut, y_train)\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)\n\n\n#Best Hyperparameters\nmy_model = XGBRegressor(colsample_bytree= 0.9, \n                        learning_rate =  0.03, \n                        gamma = 0.2,\n                        max_depth =  7, \n                        n_estimators = 500,\n                        min_child_weight = 3\n                        )\n\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(train_cut, y_train)\n\n\n# make predictions\npredictions = my_model.predict(test_cut)\n\n#Storing in the csv file\nsub = pd.DataFrame()\nsub['id'] = test_ID\nsub['target'] = predictions\nsub.to_csv('submission4.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Golden Song Contest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#####1.2) Golden Song Part#####\nGS_df = pd.read_csv('Desktop/StatLearning/dataset/test_golden_song/test_golden_song.csv')\n\n##Saving the ID and dropping it\nGS_ID = [0]\nGS_df.drop(\"id\", axis = 1, inplace = True)\n\n#Taking only the relevant columns\ntest_GS = GS_df[colname]\n\npredictions_GS = my_model.predict(test_GS)\n\n#Taking the average between the predictions\npredictions_GS_final = [prediction_GS.mean()]\n\n#Storing in the csv file\nsub_GS = pd.DataFrame()\nsub_GS['id'] = GS_ID\nsub_GS['target'] = predictions_GS_final\nsub_GS.to_csv('goldensong_group13.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Previous XGBRegression with lower score"},{"metadata":{"trusted":true},"cell_type":"code","source":"### 2) XGBRegression Previous version with old hyperparameters###\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Various hyper-parameters to tune\nxgb1 = XGBRegressor()\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'learning_rate': [0.01, .03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [100, 500, 1000]}\n\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True)\n\nxgb_grid.fit(train_cut, y_train)\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)\n\n\n#Best old hyperparameters\nmy_model = XGBRegressor(colsample_bytree= 0.7, \n                        learning_rate =  0.03, \n                        max_depth =  6, \n                        n_estimators = 1000\n                        )\n\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(train_cut, y_train, verbose=False)\n\n\n# make predictions\npredictions = my_model.predict(test_cut)\n\n\nsub = pd.DataFrame()\nsub['id'] = test_ID\nsub['target'] = predictions\nsub.to_csv('submission3.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) From where we started"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### 3) We actually started from here, searching for the best model among the following ones\n\nfrom sklearn.model_selection import train_test_split\nimport math\nimport sklearn.metrics as sklm\nfrom sklearn.preprocessing import RobustScaler\n\n#Splitting the data in order to have the possibility to compute the RMSE\nx_train, x_test, y_tra, y_test = train_test_split(train_cut, y_train,test_size = .3, random_state=0)\n\nscaler= RobustScaler()\n# transform \"x_train\"\nx_train = scaler.fit_transform(x_train)\n# transform \"x_test\"\nx_test = scaler.transform(x_test)\n#Transform the test set\nX_test= scaler.transform(test_cut)\n\n# Ridge\nimport sklearn.model_selection as GridSearchCV\nfrom sklearn.linear_model import Ridge\nimport sklearn.model_selection as ms\n\n#Using Ridge model and searching best alpha hyperparameter\nridge=Ridge()\nparameters= {'alpha':[x for x in range(1,101)]}\n\nridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nridge_reg.fit(x_train,y_tra)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_)\nprint(\"The best score achieved with Alpha=11 is: \",math.sqrt(-ridge_reg.best_score_))\nridge_pred=math.sqrt(-ridge_reg.best_score_)\n\n#Using Ridge with best alpha\nridge_mod=Ridge(alpha=100)\nridge_mod.fit(x_train,y_tra)\ny_pred_train=ridge_mod.predict(x_train)\ny_pred_test=ridge_mod.predict(x_test)\n\n#Computing RMSE for Ridge\nprint('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_tra, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_pred_test))))   \n\n\n# Lasso\nfrom sklearn.linear_model import Lasso\n\n\nlasso_mod=Lasso(alpha=100)\nlasso_mod.fit(x_train,y_tra)\ny_lasso_train=lasso_mod.predict(x_train)\ny_lasso_test=lasso_mod.predict(x_test)\n\n#Computing RMSE for Lasso\nprint('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_tra, y_lasso_train))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_lasso_test))))\n\n\n# ENET\nfrom sklearn.linear_model import ElasticNetCV\n\n#Tuning Hyperparamters\nalphas = [10,1,0.1,0.01,0.001,0.002,0.003,0.004,0.005,0.00054255]\nl1ratio = [0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\n\nelastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n\nelasticmod = elastic_cv.fit(x_train, y_tra.ravel())\nela_pred=elasticmod.predict(x_test)\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred))))\nprint(elastic_cv.alpha_)\nprint(elastic_cv.l1_ratio_)\n\n#Using the best hyperparamters\nelastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=10,  l1_ratio=0.1)\nelasticmod = elastic_cv.fit(x_train, y_tra.ravel())\nela_pred=elasticmod.predict(x_test)\n\n#RMSE for ENET\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred))))\nprint(elastic_cv.alpha_)\n\n# XGBRegressor\nxgb= XGBRegressor(colsample_bytree= 0.7, \n                        learning_rate =  0.03, \n                        max_depth =  6, \n                        n_estimators = 1000\n                        )\nxgmod=xgb.fit(x_train,y_tra)\nxg_pred=xgmod.predict(x_test)\n\n#RMSE for XGBRegressor\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, xg_pred))))\n\n#We also tried to mix them\nfrom sklearn.ensemble import VotingRegressor\n\nvote_mod = VotingRegressor([('Ridge', ridge_mod), ('Lasso', lasso_mod), ('Elastic', elastic_cv), \n                            ('XGBRegressor', xgb)])\nvote= vote_mod.fit(x_train, y_tra.ravel())\nvote_pred=vote.predict(x_test)\n\n#RMSE for the mix\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, vote_pred))))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}