{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) OFFICIAL VERSION USED for the Best Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#####1) OFFICIAL VERSION USED for the Best Score######\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Libraries for feature selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "#Path\n",
    "path_train = \"/kaggle/input/statistical-learning-sapienza-spring-2020/train/train.csv\"\n",
    "path_test = \"/kaggle/input/statistical-learning-sapienza-spring-2020/test/test.csv\"\n",
    "train = pd.read_csv(path_train)\n",
    "test  = pd.read_csv(path_test)\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ID = train['id']\n",
    "test_ID = test['id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for the prediction process.\n",
    "train.drop(\"id\", axis = 1, inplace = True)\n",
    "test.drop(\"id\", axis = 1, inplace = True)\n",
    "\n",
    "# Reshape train and test\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.tempo.values\n",
    "\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['tempo'], axis=1, inplace=True)\n",
    "train.drop(['tempo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEAUTURES SELECTION ### (Which actually improved our score)\n",
    "new = []\n",
    "for i in y_train: \n",
    "    i = int(i)\n",
    "    new += [i]\n",
    "X = np.array(train)\n",
    "y = np.array(new)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "X = pd.DataFrame(data=X[0:,0:],\n",
    "            index=[i for i in range(X.shape[0])],\n",
    "            columns=['f'+str(i) for i in range(X.shape[1])])\n",
    "\n",
    "#Using SelectKBest for feature selection\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=40000)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']   \n",
    "\n",
    "#Taking the top 209 important features\n",
    "best_columns = (featureScores.nlargest(209,'Score')) \n",
    "candidate_columns = list(best_columns.index.values)\n",
    "\n",
    "# Let's cut the dataframe\n",
    "colname = all_data.columns[candidate_columns]\n",
    "df1 = all_data[colname]\n",
    "\n",
    "\n",
    "# We only get the cut version \n",
    "train_cut = df1[:ntrain]\n",
    "test_cut = df1[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegression with the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBRegression ###\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Various hyper-parameters to tune\n",
    "xgb1 = XGBRegressor()\n",
    "parameters = {'objective':['reg:squarederror'],\n",
    "              'learning_rate': [0.01, 0.03, 0.05 ], #so called `eta` value\n",
    "              'max_depth': [7, 9],\n",
    "              'min_child_weight': [ 3 ,5],\n",
    "              'gamma': [ 0.2 ,0.3,  0.4 ],\n",
    "              'colsample_bytree': [ 0.8, 0.9, 1] ,\n",
    "              'n_estimators': [500]}\n",
    "\n",
    "# GridSearch for hyperparameters tuning\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(train_cut, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\n",
    "\n",
    "\n",
    "#Best Hyperparameters\n",
    "my_model = XGBRegressor(colsample_bytree= 0.9, \n",
    "                        learning_rate =  0.03, \n",
    "                        gamma = 0.2,\n",
    "                        max_depth =  7, \n",
    "                        n_estimators = 500,\n",
    "                        min_child_weight = 3\n",
    "                        )\n",
    "\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(train_cut, y_train)\n",
    "\n",
    "\n",
    "# make predictions\n",
    "predictions = my_model.predict(test_cut)\n",
    "\n",
    "#Storing in the csv file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('submission4.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden Song Contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####1.2) Golden Song Part#####\n",
    "GS_df = pd.read_csv('Desktop/StatLearning/dataset/test_golden_song/test_golden_song.csv')\n",
    "\n",
    "##Saving the ID and dropping it\n",
    "GS_ID = [0]\n",
    "GS_df.drop(\"id\", axis = 1, inplace = True)\n",
    "\n",
    "#Taking only the relevant columns\n",
    "test_GS = GS_df[colname]\n",
    "\n",
    "predictions_GS = my_model.predict(test_GS)\n",
    "\n",
    "#Taking the average between the predictions\n",
    "predictions_GS_final = [prediction_GS.mean()]\n",
    "\n",
    "#Storing in the csv file\n",
    "sub_GS = pd.DataFrame()\n",
    "sub_GS['id'] = GS_ID\n",
    "sub_GS['target'] = predictions_GS_final\n",
    "sub_GS.to_csv('goldensong_group13.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Previous XGBRegression with lower score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2) XGBRegression Previous version with old hyperparameters###\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Various hyper-parameters to tune\n",
    "xgb1 = XGBRegressor()\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [0.01, .03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [100, 500, 1000]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(train_cut, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\n",
    "\n",
    "\n",
    "#Best old hyperparameters\n",
    "my_model = XGBRegressor(colsample_bytree= 0.7, \n",
    "                        learning_rate =  0.03, \n",
    "                        max_depth =  6, \n",
    "                        n_estimators = 1000\n",
    "                        )\n",
    "\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(train_cut, y_train, verbose=False)\n",
    "\n",
    "\n",
    "# make predictions\n",
    "predictions = my_model.predict(test_cut)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('submission3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) From where we started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3) We actually started from here, searching for the best model among the following ones\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import sklearn.metrics as sklm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Splitting the data in order to have the possibility to compute the RMSE\n",
    "x_train, x_test, y_tra, y_test = train_test_split(train_cut, y_train,test_size = .3, random_state=0)\n",
    "\n",
    "scaler= RobustScaler()\n",
    "# transform \"x_train\"\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "# transform \"x_test\"\n",
    "x_test = scaler.transform(x_test)\n",
    "#Transform the test set\n",
    "X_test= scaler.transform(test_cut)\n",
    "\n",
    "# Ridge\n",
    "import sklearn.model_selection as GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "import sklearn.model_selection as ms\n",
    "\n",
    "#Using Ridge model and searching best alpha hyperparameter\n",
    "ridge=Ridge()\n",
    "parameters= {'alpha':[x for x in range(1,101)]}\n",
    "\n",
    "ridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\n",
    "ridge_reg.fit(x_train,y_tra)\n",
    "print(\"The best value of Alpha is: \",ridge_reg.best_params_)\n",
    "print(\"The best score achieved with Alpha=11 is: \",math.sqrt(-ridge_reg.best_score_))\n",
    "ridge_pred=math.sqrt(-ridge_reg.best_score_)\n",
    "\n",
    "#Using Ridge with best alpha\n",
    "ridge_mod=Ridge(alpha=100)\n",
    "ridge_mod.fit(x_train,y_tra)\n",
    "y_pred_train=ridge_mod.predict(x_train)\n",
    "y_pred_test=ridge_mod.predict(x_test)\n",
    "\n",
    "#Computing RMSE for Ridge\n",
    "print('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_tra, y_pred_train))))\n",
    "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_pred_test))))   \n",
    "\n",
    "\n",
    "# Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "lasso_mod=Lasso(alpha=100)\n",
    "lasso_mod.fit(x_train,y_tra)\n",
    "y_lasso_train=lasso_mod.predict(x_train)\n",
    "y_lasso_test=lasso_mod.predict(x_test)\n",
    "\n",
    "#Computing RMSE for Lasso\n",
    "print('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_tra, y_lasso_train))))\n",
    "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_lasso_test))))\n",
    "\n",
    "\n",
    "# ENET\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "#Tuning Hyperparamters\n",
    "alphas = [10,1,0.1,0.01,0.001,0.002,0.003,0.004,0.005,0.00054255]\n",
    "l1ratio = [0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\n",
    "\n",
    "elastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n",
    "\n",
    "elasticmod = elastic_cv.fit(x_train, y_tra.ravel())\n",
    "ela_pred=elasticmod.predict(x_test)\n",
    "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred))))\n",
    "print(elastic_cv.alpha_)\n",
    "print(elastic_cv.l1_ratio_)\n",
    "\n",
    "#Using the best hyperparamters\n",
    "elastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=10,  l1_ratio=0.1)\n",
    "elasticmod = elastic_cv.fit(x_train, y_tra.ravel())\n",
    "ela_pred=elasticmod.predict(x_test)\n",
    "\n",
    "#RMSE for ENET\n",
    "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred))))\n",
    "print(elastic_cv.alpha_)\n",
    "\n",
    "# XGBRegressor\n",
    "xgb= XGBRegressor(colsample_bytree= 0.7, \n",
    "                        learning_rate =  0.03, \n",
    "                        max_depth =  6, \n",
    "                        n_estimators = 1000\n",
    "                        )\n",
    "xgmod=xgb.fit(x_train,y_tra)\n",
    "xg_pred=xgmod.predict(x_test)\n",
    "\n",
    "#RMSE for XGBRegressor\n",
    "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, xg_pred))))\n",
    "\n",
    "#We also tried to mix them\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "vote_mod = VotingRegressor([('Ridge', ridge_mod), ('Lasso', lasso_mod), ('Elastic', elastic_cv), \n",
    "                            ('XGBRegressor', xgb)])\n",
    "vote= vote_mod.fit(x_train, y_tra.ravel())\n",
    "vote_pred=vote.predict(x_test)\n",
    "\n",
    "#RMSE for the mix\n",
    "print('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, vote_pred))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
